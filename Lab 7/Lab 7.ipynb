{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below writes random data with distributions of zeroes and ones according to frequencies captured in `probabilities`. 100MB files of various distributions are written to files captured in `filenames`. Therefore, each binary file has 800Mb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "filenames = [\"zeros_100p\", \"zeros_90p\", \"zeros_80p\", \"zeros_70p\", \"zeros_60p\", \"zeros_50p\"]\n",
    "probabilities = [[1, 0], [0.9, 0.1], [0.8, 0.2], [0.7, 0.3], [0.6, 0.4], [0.5, 0.5]]\n",
    "for i in range(6):\n",
    "    data = np.random.choice([0, 1], size=800*10**6, replace=True, p=probabilities[i])\n",
    "    data = np.packbits(data)\n",
    "    open(filenames[i], \"wb\") .write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the code below writes random data with uniform distributions for a DNA sequence and a protein sequence, writing to files in `filenames` and equal frequencies in `probabilities`. Both sequences are 100 million letters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"nt_seq.fa\", \"aa_seq.fa\"]\n",
    "nt_bases = [\"A\", \"T\", \"C\", \"G\"]\n",
    "aas = [\"A\", \"R\", \"N\", \"D\", \"C\", \"E\", \"Q\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "symbols = [nt_bases, aas]\n",
    "for i in range(2):\n",
    "    symbol_list = symbols[i]\n",
    "    num_symbols = len(symbol_list)\n",
    "    data = np.random.choice(symbol_list, size=100*10**6, replace=True, p=[1/num_symbols for _ in range(num_symbols)])\n",
    "    open(filenames[i], \"w\").write(\"\".join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing the data\n",
    "\n",
    "The terminal commands below compress the data using the `gzip`, `bzip2`, and `pbzip2` algorithms for each data file generated in the previous section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`time gzip –c zeros_100p > zeros_100p.gz`\n",
    "\n",
    "`time bzip2 –k zeros_100p`\n",
    "\n",
    "`time pbzip2 –k zeros_100p`\n",
    "\n",
    "`time gzip –c zeros_90p > zeros_90p.gz`\n",
    "\n",
    "`time bzip2 –k zeros_90p`\n",
    "\n",
    "`time pbzip2 –k zeros_90p`\n",
    "\n",
    "`time gzip –c zeros_80p > zeros_80p.gz`\n",
    "\n",
    "`time bzip2 –k zeros_80p`\n",
    "\n",
    "`time pbzip2 –k zeros_80p`\n",
    "\n",
    "`time gzip –c zeros_70p > zeros_70p.gz`\n",
    "\n",
    "`time bzip2 –k zeros_70p`\n",
    "\n",
    "`time pbzip2 –k zeros_70p`\n",
    "\n",
    "`time gzip –c zeros_60p > zeros_60p.gz`\n",
    "\n",
    "`time bzip2 –k zeros_60p`\n",
    "\n",
    "`time pbzip2 –k zeros_60p`\n",
    "\n",
    "`time gzip –c zeros_50p > zeros_50p.gz`\n",
    "\n",
    "`time bzip2 –k zeros_50p`\n",
    "\n",
    "`time pbzip2 –k zeros_50p`\n",
    "\n",
    "`time gzip -c nt_seq.fa > nt_seq.fa.gz`\n",
    "\n",
    "`time bzip2 –k nt_seq.fa`\n",
    "\n",
    "`time pbzip2 –k nt_seq.fa`\n",
    "\n",
    "`time gzip -c aa_seq.fa > aa_seq.fa.gz`\n",
    "\n",
    "`time bzip2 –k aa_seq.fa`\n",
    "\n",
    "`time pbzip2 –k aa_seq.fa`\n",
    "\n",
    "|  Input File  | Input File Size | Compression Algorithm | Output File Size | Runtime (gzip, bzip2, pbzip2) |\n",
    "|:------------:|:---------------:|:---------------------:|:----------------:|:-----------------------------:|\n",
    "| `zeros_100p` |      100MB      |          `gzip`         |      97.1kB      |            0m0.577s           |\n",
    "| `zeros_100p` |      100MB      |         `bzip2`         |       113B       |            0m0.863s           |\n",
    "| `zeros_100p` |      100MB      |         `pbzip2`        |      5.38kB      |            0m0.136s           |\n",
    "|  `zeros_90p` |      100MB      |          `gzip`         |       56MB       |           0m25.736s           |\n",
    "|  `zeros_90p` |      100MB      |         `bzip2`         |      58.3MB      |            0m9.497s           |\n",
    "|  `zeros_90p` |      100MB      |         `pbzip2`        |      58.4MB      |            0m1.096s           |\n",
    "|  `zeros_80p` |      100MB      |          `gzip`         |      77.4MB      |           0m16.638s           |\n",
    "|  `zeros_80p` |      100MB      |         `bzip2`         |      82.6MB      |            0m9.992s           |\n",
    "|  `zeros_80p` |      100MB      |         `pbzip2`        |      82.6MB      |            0m1.280s           |\n",
    "|  `zeros_70p` |      100MB      |          `gzip`         |      89.3MB      |            0m6.787s           |\n",
    "|  `zeros_70p` |      100MB      |         `bzip2`         |      59.3MB      |           0m10.901s           |\n",
    "|  `zeros_70p` |      100MB      |         `pbzip2`        |      95.1MB      |            0m1.422s           |\n",
    "| `zeros_60p` |      100MB      |          `gzip`         |      97.7MB      |            0m4.217s           |\n",
    "| `zeros_60p` |      100MB      |         `bzip2`         |       100MB      |           0m11.988s           |\n",
    "| `zeros_60p` |      100MB      |         `pbzip2`        |       100MB      |            0m1.682s           |\n",
    "| `zeros_50p` |      100MB      |          `gzip`         |       100MB      |            0m3.404s           |\n",
    "| `zeros_50p` |      100MB      |         `bzip2`         |       100MB      |           0m12.621s           |\n",
    "| `zeros_50p` |      100MB      |         `pbzip2`        |       100MB      |            0m1.806s           |\n",
    "| `nt_seq.fa` |      100MB      |          `gzip`         |      29.2MB      |           0m22.989s           |\n",
    "| `nt_seq.fa` |      100MB      |         `bzip2`         |      27.3MB      |            0m9.246s           |\n",
    "| `nt_seq.fa` |      100MB      |         `pbzip2`        |      27.3MB      |            0m1.048s           |\n",
    "| `aa_seq.fa` |      100MB      |          `gzip`         |      60.6MB      |            0m4.233s           |\n",
    "| `aa_seq.fa` |      100MB      |         `bzip2`         |      55.2MB      |            0m9.668s           |\n",
    "| `aa_seq.fa` |      100MB      |         `pbzip2`        |      55.3MB      |            0m1.019s           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Which algorithm achieves the best level of compression on each file type?**\n",
    "\n",
    "For bit strings: `zeros_100p`: `bzip2` is best, `zeros_90p`: `gzip` is best, `zeros_80p`: `gzip` is best, `zeros_70p`: `bzip2` is best, `zeros_60p`: `gzip` is best, `zeros_50`: `gzip`, `bzip2`, `pbzip2` are equally good.\n",
    "\n",
    "For DNA: `bzip2`, `pbzip2` are equally good.\n",
    "\n",
    "For amino acids: `bzip2` is best.\n",
    "\n",
    "**Which algorithm is the fastest?**\n",
    "\n",
    "`pbzip2` is the fastest algorithm for all the files tested.\n",
    "\n",
    "**What is the difference between `bzip2` and `pbzip2`?**\n",
    "\n",
    "`pzip2` is a parallel implementation of `bzip2`; that is, `pzip2` support multi-threading, whereas `bzip2` does not.\n",
    "\n",
    "**Do you expect one to be faster and why?**\n",
    "\n",
    "I expect `pbzip2` to be faster because `pbzip2` can execute the compression sub-tasks simultaneously by using multiple CPUs, but `bzip2` cannot.\n",
    "\n",
    "**How does the level of compression change as the percentage of zeros increases?**\n",
    "\n",
    "The level of compression increases as the percentage of zeros increases from $50\\%$ to $100\\%$.\n",
    "\n",
    "**Why does this happen?** \n",
    "\n",
    "This happens because since the zeros occur with a greater frequency, so the Shannon entropy decreases. \n",
    "\n",
    "In particular, since `gzip` relies on Huffman coding and the LZ1 algorithm, it can achieve a greater level of compression when zeros are more frequent by using shorter codewords and recording a greater number of repeated sequences.\n",
    "\n",
    "**What is the minimum number of bits required to store a single DNA base?**\n",
    "\n",
    "Assuming that DNA bases are uniformly distributed and that we are looking for a prefix-free code, a minimum of 2 bits are required.\n",
    "\n",
    "**What is the minimum number of bits required to store an amino acid letter?**\n",
    "\n",
    "Assuming that amino acid letters are uniformly distributed and that we are looking for a prefix-free code, a minimum of $\\log_{2}{20} \\approx 4.322$ bits are required.\n",
    "\n",
    "**In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and protein sequences?** \n",
    "\n",
    "For DNA: `gzip` required 29.2MB, or $29.2\\text{MB} \\times 8\\cdot10^6 \\frac{\\text{bits}}{\\text{MB}} = 233600000 \\text{ bits} 2.336\\cdot10^8 \\text{ bits}$ and `bzip2` required 27.3MB, or $27.3\\text{MB} \\times 8\\cdot10^6 \\frac{\\text{bits}}{\\text{MB}} = 218400000 \\text{ bits} = 2.184\\cdot10^8 \\text{ bits} $.\n",
    "\n",
    "For amino acids: `gzip` required 60.6MB, or $60.6\\text{MB} \\times 8\\cdot10^6 \\frac{\\text{bits}}{\\text{MB}} = 484800000 \\text{ bits} = 4.848\\cdot10^8 \\text{ bits}$ and `bzip2` required 55.2MB, or $55.2\\text{MB} \\times 8\\cdot10^6 \\frac{\\text{bits}}{\\text{MB}} = 441600000 \\text{ bits} = 4.416\\cdot10^8 \\text{ bits}$.\n",
    "\n",
    "**Are gzip and bzip2 performing well on DNA and proteins?**\n",
    "\n",
    "For DNA: 100 million bases should ideally require $100\\times10^6\\cdot2 = 2\\cdot10^8 \\text{ bits}$. `gzip` requires $4.848\\cdot10^8 \\text{ bits}$, so the percent wasteage is $\\frac{4.848\\cdot10^8 - 2\\cdot10^8}{2\\cdot10^8} \\cdot 100\\%= 16.8\\%$. `bzip2` requires $2.184\\cdot10^8 \\text{ bits}$, so the percent wastage is $\\frac{2.184\\cdot10^8 - 2\\cdot10^8}{2\\cdot10^8} \\cdot 100\\%= 9.2\\%$. `bzip2` performs slightly better than `gzip`. In both cases, the percent wastage is generally small; therefore, `gzip` and `bzip2` are performing well on DNA.\n",
    "\n",
    "For amino acids: 100 million letters should ideally require $100\\times10^6\\cdot4.322 = 4.322\\cdot10^8 \\text{ bits}$. `gzip` requires $4.848\\cdot10^8 \\text{ bits}$, so the percent wastage is $\\frac{4.848\\cdot10^8 - 4.322\\cdot10^8}{4.322\\cdot10^8} \\cdot 100\\%\\approx 12.170\\%$. `bzip2` requires $4.416\\cdot10^8 \\text{ bits}$, so the percent wastage is $\\frac{4.416\\cdot10^8 - 4.322\\cdot10^8}{4.322\\cdot10^8} \\cdot 100\\%\\approx 2.175\\%$. `bzip2` performs significantly better than `gzip`. In both cases, the percent wastage is generally small; therefore, `gzip` and `bzip2` are performing well on amino acids, or proteins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below searches the Entrez database for the DNA sequences of gp120 homologs from HIV isolates, and then combines them into a multi-FASTA (`hiv_gp120homologs.fa`) for processing. We take care to concatenate at least 10 of these homologs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9921 bases in the FASTA file.\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Alphabet import IUPAC\n",
    "\n",
    "Entrez.email = \"sjclin@berkeley.edu\"\n",
    "\n",
    "# Find the nucleic acid sequences of gp120 homologs from at least 10 different HIV isolates\n",
    "handle = Entrez.esearch(db=\"nucleotide\", term=\"gp120 HIV\", sort=\"relevance\", idtype=\"acc\")\n",
    "IdList = Entrez.read(handle)[\"IdList\"]\n",
    "assert len(IdList) >= 10\n",
    "\n",
    "num_bases = 0\n",
    "multi_fasta = open(\"hiv_gp120homologs.fa\", \"a+\")\n",
    "for i in IdList:\n",
    "    handle = Entrez.efetch(db=\"nucleotide\", id=i, rettype=\"fasta\", retmode=\"text\")\n",
    "    # Concatenate them together into a single multi-FASTA\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        num_bases += len(seq)\n",
    "        multi_fasta.write(seq)\n",
    "print(\"There are \" + str(num_bases) + \" bases in the FASTA file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A priori, do you expect to achieve better or worse compression here than random data?** **Why?**\n",
    "\n",
    "I expect to achieve better compresion here than random data. In random data, the symbols are approximately uniformly distributed. However, in real data, there likely exists patterns, i.e. some characters are more common than others. This means that the algorithms can exploit the skewed distributions to achieve better compression than compared to random data.\n",
    "\n",
    "The terimnal commands below compress `hiv_gp120homologs.fa` (which contains the data from the gp120 homologs) using `gzip` and `bzip2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gzip -c hiv_gp120homologs.fa > hiv_gp120homologs.fa.gz`\n",
    "\n",
    "`bzip2 -k hiv_gp120homologs.fa`\n",
    "\n",
    "|      Input File      | Input File Size | Compression Algorithm | Output File Size | Runtime (gzip, bzip2, pbzip2) |\n",
    "|:--------------------:|:---------------:|:---------------------:|:----------------:|:-----------------------------:|\n",
    "| hiv_gp120homologs.fa |      8.44kB     |          gzip         |       968B       |            0m0.001s           |\n",
    "| hiv_gp120homologs.fa |      8.44kB     |         bzip2         |      1.08kB      |            0m0.002s           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the compression ratio of this file compare to random data?** \n",
    "\n",
    "`gzip` compressed this file to $\\frac{968}{8.44\\cdot10^3}\\cdot100\\% \\approx 11.469\\%$ of the original size, whereas it compressed the random data file to $\\frac{29.2\\cdot10^6}{100\\cdot10^6} \\cdot100\\% = 29.2\\%$ of its original size. We have that $11.469\\%<29.2\\%$, so the compression ratio is better for `hiv_gp120homologs.fa`.\n",
    "\n",
    "`bzip2` compressed this file to $\\frac{1.08\\cdot10^3}{8.44\\cdot10^3}\\cdot100\\% \\approx 12.796\\%$ of the original size, whereas it compressed the random data file to $\\frac{27.3\\cdot10^6}{100\\cdot10^6} \\cdot100\\% = 27.3\\%$ of its original size. We have that $12.796 < 27.3\\%$, so the compression ratio is better for `hiv_gp120homologs.fa`.\n",
    "\n",
    "*In both cases, the compression ratio of `hiv_gp120homologs.fa` is better than that of random data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating compression of 1000 terabytes\n",
    "\n",
    "> Let’s make some assumptions about the contents of the data at your biotech company. Most of the data, say 80%, is re-sequencing of genomes and plasmids that are very similar to each other. Another 10% might be protein sequences, and the last 10% are binary microscope images which we’ll assume follow the worst-case scenario of being completely random. \n",
    "\n",
    "**Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data?** \n",
    "\n",
    "For the genomes and plasmids, we should use `gzip`, as it is faster than `bzip2` for non-random DNA and achieves a higher level of compression than `bzip2`.\n",
    "\n",
    "For the protein sequences, we should use `pbzip2`, as it is the fastest algorithm for amino acids and achieves only a slightly lower level of compression compared to `bzip2` (`pbzip2` compression is ($\\frac{55.3\\cdot10^6 - 55.2\\cdot10^6}{55.2\\cdot10^6}\\cdot100\\% \\approx 0.181\\%$) larger), yet `bzip2` takes over 9 times longer.\n",
    "\n",
    "For the binary microscope images, we should use `pbzip2`, as it is the fastest algorithm for random binary data, and the algorithms we tested achieve equal amounts of compression.\n",
    "\n",
    "**Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to compute how much time the algorithms will take to run according to our benchmarks, assuming that they run in $O(n)$ time.\n",
    "\n",
    "For the genomes and plasmids, `gzip` will take approximately $\\frac{0.8\\times1000\\cdot10^{12}}{8.44\\cdot10^3}\\times 0.001\\text{s} \\approx 9.479 \\cdot 10^7\\text{s}$.\n",
    "\n",
    "For the protein sequences, `pbzip2` will take approximately $\\frac{0.1\\times1000\\cdot10^{12}}{100\\cdot10^6}\\times1.019\\text{s} = 1.019\\cdot10^7\\text{s}$.\n",
    "\n",
    "For the binary microscope images, `pbzip2` will take approximately $\\frac{0.1\\times1000\\cdot10^{12}}{100\\cdot10^6}\\times1.806\\text{s} = 1.806\\cdot10^7\\text{s}$.\n",
    "\n",
    "There are $8.64 \\cdot 10^4\\text{s}$ in a day. We will not have enough time to compress all of the data. We will look to save as much space as possible given the time constraints. \n",
    " \n",
    "On the genomes and plasmids, `gzip` can compress $\\frac{8440-968}{0.001}\\frac{\\text{B}}{\\text{s}} = 7.472\\cdot10^6\\frac{\\text{B}}{\\text{s}}$.\n",
    "\n",
    "On the protein sequences, `pbzip2` can compress $\\frac{100\\cdot10^6-55.3\\cdot10^6}{1.019}\\frac{\\text{B}}{\\text{s}} \\approx 4.387\\cdot10^7\\frac{\\text{B}}{\\text{s}}$.\n",
    "\n",
    "On the binary microscope images, `pbzip2` can compress $\\frac{100\\cdot10^6-100\\cdot10^6}{1.806}\\frac{\\text{B}}{\\text{s}} = 0\\frac{\\text{B}}{\\text{s}}$.\n",
    "\n",
    "*We should use `pbzip2` on the protein sequences to save as much space as possible. In the span of a day, we can save $\\frac{44.7\\cdot10^6}{1.019}\\frac{\\text{B}}{\\text{s}}\\times 8.64 \\cdot 10^4\\text{s} \\approx 3.79 \\cdot 10^{12} \\text{B} = 3.79 \\text{TB}$. We can save approximately $\\frac{3.79}{1000}\\times100\\% = 0.379\\%$ of space each day, so the bonus per day is $0.379\\% \\times \\frac{\\$ 500}{1\\%} = \\$ 189$ since we generate 1000TB each day. Therefore, the bonus for the entire year will be about $\\frac{\\$ 189}{1 \\text{ day}} \\times 365 \\text{ days} = \\$ 68985$.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
